{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec088f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\cocsa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Task 3: RAG - Part 2 (Memory + Multi-step Retrieval) with Groq - Colab Version\n",
    "# Objective: Enable deeper QA with memory and multi-turn dialogue using Groq\n",
    "\n",
    "# ‚úÖ Step 1: Install dependencies in Colab\n",
    "!pip install -q langchain langchain-community faiss-cpu pypdf langchain-groq gradio huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4050e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 2: Import all required libraries\n",
    "import os\n",
    "import gradio as gr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import BaseOutputParser\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# üîë Step 3: Set up API Key\n",
    "# =====================================\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b69eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from 1 PDF files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Step 4: Load PDFs from local folder\n",
    "folder_path = \"PDFSAMPLE/\"  # folder containing your PDFs\n",
    "pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "documents = []\n",
    "for path in pdf_files:\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {len(pdf_files)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c397e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating new FAISS index from uploaded PDFs...\n",
      "‚úÖ Loaded 1 pages from SK Cover letterr_compressed (1).pdf\n",
      "üìÑ Total documents loaded: 1\n",
      "üî™ Split into 2 chunks\n",
      "‚úÖ FAISS index created and saved\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# üìÑ Step 5: Load and Process PDFs\n",
    "# =====================================\n",
    "def load_and_process_pdfs():\n",
    "    \"\"\"Load PDFs and create vector store\"\"\"\n",
    "    folder_path = \"PDFSAMPLE/\"  # folder containing your PDFs\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No PDF files found. Please upload PDFs first.\")\n",
    "        return None\n",
    "    \n",
    "    documents = []\n",
    "    for path in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            print(f\"‚úÖ Loaded {len(docs)} pages from {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {path}: {e}\")\n",
    "    \n",
    "    print(f\"üìÑ Total documents loaded: {len(documents)}\")\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"üî™ Split into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "    db.save_local(\"faiss_index\")\n",
    "    print(\"‚úÖ FAISS index created and saved\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "# Create or load vector store\n",
    "try:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"‚úÖ FAISS index loaded from disk\")\n",
    "except:\n",
    "    print(\"üîÑ Creating new FAISS index from uploaded PDFs...\")\n",
    "    db = load_and_process_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "519ce1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MultiQuery Retriever with Groq configured\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# üîÑ Step 6: Set Up MultiQuery Retriever with Groq\n",
    "# =====================================\n",
    "class LineListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Output parser that parses a string into a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Set up Groq LLM for retrieval\n",
    "llm_for_retrieval = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Fast model for retrieval\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Create MultiQuery Retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    llm=llm_for_retrieval,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MultiQuery Retriever with Groq configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635ade99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational QA Chain with Memory configured\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# üß† Step 7: Set Up Memory + Conversation Chain\n",
    "# =====================================\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Enhanced custom prompt template for manufacturing context\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "    template=(\n",
    "        \"You are an expert manufacturing engineer and technical documentation specialist. \"\n",
    "        \"Your role is to provide accurate, detailed answers based on the provided technical documents, \"\n",
    "        \"manuals, ISO standards, and SOPs.\\n\\n\"\n",
    "        \n",
    "        \"IMPORTANT INSTRUCTIONS:\\n\"\n",
    "        \"1. Use ONLY the document context below to answer the question\\n\"\n",
    "        \"2. Consider the chat history for context and follow-up questions\\n\"\n",
    "        \"3. For comparative questions, highlight similarities and differences clearly\\n\"\n",
    "        \"4. Reference specific procedures, standards, or safety requirements\\n\"\n",
    "        \"5. If information is not in the context, say 'Based on the provided documents, this information is not available'\\n\"\n",
    "        \"6. Provide comprehensive but concise answers\\n\\n\"\n",
    "        \n",
    "        \"CHAT HISTORY:\\n{chat_history}\\n\\n\"\n",
    "        \"DOCUMENT CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION: {question}\\n\\n\"\n",
    "        \"EXPERT ANSWER:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set up Groq LLM for conversation\n",
    "conversation_llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",  # High-quality model for responses\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# Create conversational retrieval chain\n",
    "conversational_qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=conversation_llm,\n",
    "    retriever=multi_query_retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational QA Chain with Memory configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "805467bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üí¨ Step 8: Enhanced Chat Function for Colab\n",
    "# =====================================\n",
    "def chat_with_memory(question, chat_history=None):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a valid question.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîç Processing: {question[:50]}...\")\n",
    "        \n",
    "        # Execute the conversational chain\n",
    "        result = conversational_qa_chain({\"question\": question})\n",
    "        answer = result[\"answer\"]\n",
    "        \n",
    "        # Extract source information\n",
    "        source_docs = result.get(\"source_documents\", [])\n",
    "        sources = []\n",
    "        for i, doc in enumerate(source_docs[:3]):\n",
    "            source_name = doc.metadata.get('source', 'Unknown')\n",
    "            page = doc.metadata.get('page', 'N/A')\n",
    "            sources.append(f\"üìÑ {os.path.basename(source_name)} (Page {page+1})\")\n",
    "        \n",
    "        source_text = \"\\n\".join(sources) if sources else \"No specific sources identified\"\n",
    "        \n",
    "        return answer, source_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return f\"‚ö†Ô∏è Error processing your question: {str(e)}\", \"\"\n",
    "\n",
    "def clear_memory():\n",
    "    global conversational_qa_chain\n",
    "    conversational_qa_chain.memory.clear()\n",
    "    return \"üóëÔ∏è Conversation history cleared!\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0f5927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cocsa\\AppData\\Local\\Temp\\ipykernel_9700\\2608431039.py:26: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# üé® Step 9: Build Colab-Optimized Gradio Interface\n",
    "# =====================================\n",
    "def create_colab_interface():\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Manufacturing RAG - Colab\") as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # üè≠ Manufacturing Document QA - Colab\n",
    "            ## Powered by Groq + Memory + Multi-Query Retrieval\n",
    "            \n",
    "            **Features:**\n",
    "            - ‚úÖ Multi-turn conversations with memory\n",
    "            - ‚úÖ Comparative analysis across documents  \n",
    "            - ‚úÖ Source document tracking\n",
    "            - ‚úÖ Colab-optimized performance\n",
    "            \n",
    "            **Try this workflow:**\n",
    "            1. \"What are maintenance steps for equipment A?\"\n",
    "            2. \"Compare with equipment B maintenance\"\n",
    "            3. \"Which has stricter safety requirements?\"\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Expert Chat\",\n",
    "                    height=400,\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    question_input = gr.Textbox(\n",
    "                        label=\"Ask about manuals, SOPs, standards...\",\n",
    "                        placeholder=\"e.g., What are the safety procedures for operating heavy machinery?\",\n",
    "                        scale=4\n",
    "                    )\n",
    "                    submit_btn = gr.Button(\"üöÄ Ask\", variant=\"primary\", scale=1)\n",
    "                \n",
    "                with gr.Row():\n",
    "                    clear_btn = gr.Button(\"üóëÔ∏è Clear History\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                sources_output = gr.Textbox(\n",
    "                    label=\"üìö Source Documents\",\n",
    "                    lines=8,\n",
    "                    interactive=False\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"### üí° Example Questions\")\n",
    "                \n",
    "                examples = [\n",
    "                    \"What safety equipment is required?\",\n",
    "                    \"Explain maintenance schedules\",\n",
    "                    \"Compare different machine procedures\",\n",
    "                    \"What ISO standards are referenced?\"\n",
    "                ]\n",
    "                \n",
    "                for example in examples:\n",
    "                    gr.Button(example, size=\"sm\").click(\n",
    "                        lambda x=example: x, \n",
    "                        outputs=question_input\n",
    "                    )\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, history):\n",
    "            answer, sources = chat_with_memory(message)\n",
    "            history.append((message, answer))\n",
    "            return \"\", history, sources\n",
    "        \n",
    "        question_input.submit(\n",
    "            respond,\n",
    "            [question_input, chatbot],\n",
    "            [question_input, chatbot, sources_output]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            [question_input, chatbot],\n",
    "            [question_input, chatbot, sources_output]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            lambda: ([], \"No active sources\"),\n",
    "            outputs=[chatbot, sources_output]\n",
    "        ).then(clear_memory)\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create the interface\n",
    "colab_ui = create_colab_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "695947f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting Gradio interface...\n",
      "üì± The interface will open below. For public access, use the public URL provided.\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing: who is suraj...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cocsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================\n",
    "# üöÄ Step 10: Launch in Colab with Public URL\n",
    "# =====================================\n",
    "print(\"üéØ Starting Gradio interface...\")\n",
    "print(\"üì± The interface will open below. For public access, use the public URL provided.\")\n",
    "\n",
    "# Launch with share=True for public access\n",
    "colab_ui.launch(\n",
    "    share=True,  # Creates public URL\n",
    "    debug=True,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e7d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üß™ Step 11: Test Functionality\n",
    "# =====================================\n",
    "def quick_test():\n",
    "    \"\"\"Quick test of the system\"\"\"\n",
    "    print(\"üß™ Running quick test...\")\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What are the key safety procedures mentioned?\",\n",
    "        \"What types of equipment are discussed in the documents?\"\n",
    "    ]\n",
    "    \n",
    "    for q in test_questions:\n",
    "        print(f\"\\nüë§ Q: {q}\")\n",
    "        answer, sources = chat_with_memory(q)\n",
    "        print(f\"üè≠ A: {answer[:100]}...\")\n",
    "        print(f\"üìö Sources: {sources}\")\n",
    "\n",
    "# Uncomment to run test\n",
    "# quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cfba224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä System Information:\n",
      "‚úÖ GROQ_API_KEY: Set\n",
      "‚úÖ FAISS Index: Loaded\n",
      "‚úÖ PDF Files: 1\n",
      "‚úÖ Memory: Active\n",
      "‚úÖ Groq API: Connected\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# üìä Step 12: System Information\n",
    "# =====================================\n",
    "def system_info():\n",
    "    \"\"\"Display system information\"\"\"\n",
    "    print(\"üìä System Information:\")\n",
    "    print(f\"‚úÖ GROQ_API_KEY: {'Set' if os.getenv('GROQ_API_KEY') else 'Not set'}\")\n",
    "    print(f\"‚úÖ FAISS Index: {'Loaded' if db else 'Not loaded'}\")\n",
    "    print(f\"‚úÖ PDF Files: {len([f for f in os.listdir('PDFSAMPLE/') if f.endswith('.pdf')])}\")\n",
    "    print(f\"‚úÖ Memory: {'Active' if conversational_qa_chain.memory else 'Inactive'}\")\n",
    "    \n",
    "    # Test API connection\n",
    "    try:\n",
    "        test_llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "        test_llm.invoke(\"Hello\")\n",
    "        print(\"‚úÖ Groq API: Connected\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Groq API: Error - {e}\")\n",
    "\n",
    "system_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
